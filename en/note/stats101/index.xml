<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Welcome to stat101 | MDneural.net</title>
    <link>/en/note/stats101/</link>
      <atom:link href="/en/note/stats101/index.xml" rel="self" type="application/rss+xml" />
    <description>Welcome to stat101</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>MDneural.net © 2020</copyright><lastBuildDate>Tue, 12 Nov 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/en/note/stats101/featured.png</url>
      <title>Welcome to stat101</title>
      <link>/en/note/stats101/</link>
    </image>
    
    <item>
      <title>Probability Redux</title>
      <link>/en/note/stats101/1.post1/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      <guid>/en/note/stats101/1.post1/</guid>
      <description>

&lt;p&gt;앞으로 통계 공부를 하는데 있어서 중요한 이론들을 다시 살펴보도록 하자&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;laws-of-large-numbers-큰-수의-법칙&#34;&gt;Laws of Large Numbers (큰 수의 법칙)&lt;/h2&gt;

&lt;p&gt;확률변수 $X_1, X_2,&amp;hellip;,X_n\sim^{i.i.d} X$에 대해 $\mu = \sum [X]$,  $\sigma^2 = Var[X]$ 라면, 아래의 식이 성립한다.&lt;/p&gt;

&lt;p&gt;$$
\bar X_n:=\frac{1}{n}\sum _{i=1}^ n X_i \xrightarrow [n\to \infty ]{\mathbf{P},\mbox{ a.s.}} \mu
$$&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;central-limit-theorem-중심극한-정리&#34;&gt;Central Limit theorem (중심극한 정리)&lt;/h2&gt;

&lt;p&gt;확률변수 $X_1, X_2,&amp;hellip;,X_n\sim^{i.i.d} X$에 대해 $\mu = \sum[X]$, $\sigma^2 = Var[X]$ 라면, 아래의 식이 성립한다.&lt;/p&gt;

&lt;p&gt;$$
\displaystyle \sqrt{n}\, \frac{\bar X_ n-\mu }{\sigma } \quad \displaystyle \xrightarrow [n\to \infty ]{(d)} \quad \displaystyle \mathcal{N}(0,1)&lt;br /&gt;
$$&lt;/p&gt;

&lt;p&gt;$$
\text{or equivalently, }\displaystyle \sqrt{n}\, (\bar X_ n-\mu) \quad \displaystyle \xrightarrow [n\to \infty ]{(d)} \quad \displaystyle \mathcal{N}(0,\sigma^2)
$$&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;hoeffding-s-inequality-호에프딩-부등식&#34;&gt;Hoeffding&amp;rsquo;s Inequality (호에프딩 부등식)&lt;/h2&gt;

&lt;p&gt;주어진 $n (n&amp;gt;0)$에 대하여 동일한 분포를 갖는 독립확률변수 $X_1, X_2,&amp;hellip;,X_n\sim^{i.i.d} X$가 거의 확실하게 수렴(almost surely convergence) 한다면, 다시 말해 $P(X \notin [a,b])=0,\quad a,b\ne\infty$ 라면&lt;/p&gt;

&lt;p&gt;모든 $\epsilon \gt 0$에 대해 아래의 식이 성립한다.
$$
P(|\overline{X_n} - \sum[X]| \geq \epsilon) \leq 2\exp\Bigg(-\frac{2n\epsilon^2}{(b-a)^2}\Bigg)
$$&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;중심극한정리와는 다르게 &lt;strong&gt;Hoeffding&amp;rsquo;s Inequality에서는 n이 클 필요가 없다.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;markov-inequality-마코브-부등식&#34;&gt;Markov Inequality (마코브 부등식)&lt;/h2&gt;

&lt;p&gt;확률변수 $X \geq 0$ 의 평균 $\mu &amp;gt; 0$이면 모든 $t&amp;gt;0$에 대해  아래의 식이 성립한다.
$$
P(X \geq t) \leq \frac{\mu}{t}.
$$&lt;/p&gt;

&lt;p&gt;&lt;i&gt;Note: 마크로브 부등식은 0이상의 확률변수에 대해서만 성립한다.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;chebyshev-inequality-체비쇼프-부등식&#34;&gt;Chebyshev Inequality (체비쇼프 부등식)&lt;/h2&gt;

&lt;p&gt;평균이 $\mu$이고 분산이 $\sigma^2$인 확률변수 $X$는 모든 $t&amp;gt;0$에 대해  아래의 식이 성립한다.
$$
P(|X - \mu| \geq t) \leq \frac{\sigma^2}{t^2}
$$&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Markov inequality에 $(X-\mu)^2$를 적용하면 Chebyshev inequality를 구할 수 있다.
뿐만아니라 Markov inequality는 Hoeffding&amp;rsquo;s inequality를 증명할때 사용되기도 한다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>
