<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MDneural.net</title>
    <link>/en/</link>
      <atom:link href="/en/index.xml" rel="self" type="application/rss+xml" />
    <description>MDneural.net</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>MDneural.net © 2020</copyright><lastBuildDate>Wed, 04 Mar 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/logo.png</url>
      <title>MDneural.net</title>
      <link>/en/</link>
    </image>
    
    <item>
      <title>Probability Redux</title>
      <link>/en/note/stats101/1.post1/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      <guid>/en/note/stats101/1.post1/</guid>
      <description>

&lt;p&gt;앞으로 통계 공부를 하는데 있어서 중요한 이론들을 다시 살펴보도록 하자&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;laws-of-large-numbers-큰-수의-법칙&#34;&gt;Laws of Large Numbers (큰 수의 법칙)&lt;/h2&gt;

&lt;p&gt;확률변수 $X_1, X_2,&amp;hellip;,X_n\sim^{i.i.d} X$에 대해 $\mu = \sum [X]$,  $\sigma^2 = Var[X]$ 라면, 아래의 식이 성립한다.&lt;/p&gt;

&lt;p&gt;$$
\bar X_n:=\frac{1}{n}\sum _{i=1}^ n X_i \xrightarrow [n\to \infty ]{\mathbf{P},\mbox{ a.s.}} \mu
$$&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;central-limit-theorem-중심극한-정리&#34;&gt;Central Limit theorem (중심극한 정리)&lt;/h2&gt;

&lt;p&gt;확률변수 $X_1, X_2,&amp;hellip;,X_n\sim^{i.i.d} X$에 대해 $\mu = \sum[X]$, $\sigma^2 = Var[X]$ 라면, 아래의 식이 성립한다.&lt;/p&gt;

&lt;p&gt;$$
\displaystyle \sqrt{n}\, \frac{\bar X_ n-\mu }{\sigma } \quad \displaystyle \xrightarrow [n\to \infty ]{(d)} \quad \displaystyle \mathcal{N}(0,1)&lt;br /&gt;
$$&lt;/p&gt;

&lt;p&gt;$$
\text{or equivalently, }\displaystyle \sqrt{n}\, (\bar X_ n-\mu) \quad \displaystyle \xrightarrow [n\to \infty ]{(d)} \quad \displaystyle \mathcal{N}(0,\sigma^2)
$$&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;hoeffding-s-inequality-호에프딩-부등식&#34;&gt;Hoeffding&amp;rsquo;s Inequality (호에프딩 부등식)&lt;/h2&gt;

&lt;p&gt;주어진 $n (n&amp;gt;0)$에 대하여 동일한 분포를 갖는 독립확률변수 $X_1, X_2,&amp;hellip;,X_n\sim^{i.i.d} X$가 거의 확실하게 수렴(almost surely convergence) 한다면, 다시 말해 $P(X \notin [a,b])=0,\quad a,b\ne\infty$ 라면&lt;/p&gt;

&lt;p&gt;모든 $\epsilon \gt 0$에 대해 아래의 식이 성립한다.
$$
P(|\overline{X_n} - \sum[X]| \geq \epsilon) \leq 2\exp\Bigg(-\frac{2n\epsilon^2}{(b-a)^2}\Bigg)
$$&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;중심극한정리와는 다르게 &lt;strong&gt;Hoeffding&amp;rsquo;s Inequality에서는 n이 클 필요가 없다.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;markov-inequality-마코브-부등식&#34;&gt;Markov Inequality (마코브 부등식)&lt;/h2&gt;

&lt;p&gt;확률변수 $X \geq 0$ 의 평균 $\mu &amp;gt; 0$이면 모든 $t&amp;gt;0$에 대해  아래의 식이 성립한다.
$$
P(X \geq t) \leq \frac{\mu}{t}.
$$&lt;/p&gt;

&lt;p&gt;&lt;i&gt;Note: 마크로브 부등식은 0이상의 확률변수에 대해서만 성립한다.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;chebyshev-inequality-체비쇼프-부등식&#34;&gt;Chebyshev Inequality (체비쇼프 부등식)&lt;/h2&gt;

&lt;p&gt;평균이 $\mu$이고 분산이 $\sigma^2$인 확률변수 $X$는 모든 $t&amp;gt;0$에 대해  아래의 식이 성립한다.
$$
P(|X - \mu| \geq t) \leq \frac{\sigma^2}{t^2}
$$&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Markov inequality에 $(X-\mu)^2$를 적용하면 Chebyshev inequality를 구할 수 있다.
뿐만아니라 Markov inequality는 Hoeffding&amp;rsquo;s inequality를 증명할때 사용되기도 한다.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>좌충우돌 파이토치 medical imaging 라이브러리 개발기-1</title>
      <link>/en/blog/pytorch_1/</link>
      <pubDate>Wed, 04 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/en/blog/pytorch_1/</guid>
      <description>&lt;p&gt;머신러닝을 공부하고 파이썬을 사용하면서 편리했던 것 중 하나는 풍부한 라이브러리 였다. 어떤 기능이 필요해서 조금만 구글링을 해보면 이미 해당 기능을 구현해서 라이브러리로 올려놓은 천사같은 분이 있는 경우가 대부분이었다. 라이브러리를 임포트해서 쓰면서 나도 언젠가 오픈소스 라이브러리를 만들어서 많은 사람들에게 도움을 줄 수 있으면 좋겠다고 결심하곤 했었다.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/en/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/en/about/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
